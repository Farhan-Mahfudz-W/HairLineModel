{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fm2klCkz8sSu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from tkinter import Image\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.utils import Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definisi kelas PyDataset\n",
        "class PyDataset(Sequence):\n",
        "    def __init__(self, directory, batch_size, image_size, mode='train', **kwargs):\n",
        "        self.directory = directory\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.mode = mode\n",
        "        self.datagen = ImageDataGenerator(\n",
        "            rescale=1.0/255,\n",
        "            shear_range=0.2 if mode == 'train' else 0.0,\n",
        "            zoom_range=0.2 if mode == 'train' else 0.0,\n",
        "            horizontal_flip=True if mode == 'train' else False,\n",
        "            preprocessing_function=preprocess_input\n",
        "        )\n",
        "        self.generator = self.datagen.flow_from_directory(\n",
        "            self.directory,\n",
        "            target_size=self.image_size,\n",
        "            batch_size=self.batch_size,\n",
        "            class_mode='categorical'\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.generator.samples // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return next(self.generator)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.generator.on_epoch_end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiLVcXce8sSv",
        "outputId": "baa11152-1ce2-4f6b-f1e1-08ca612738d6"
      },
      "outputs": [],
      "source": [
        "# Load VGG16 without the top layer\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Pap7nJ5eiBJp"
      },
      "outputs": [],
      "source": [
        "base_path = \"Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_dir = 'Dataset'\n",
        "train_dir = os.path.join(dataset_dir, 'Train')\n",
        "validation_dir = os.path.join(dataset_dir, 'Validation')\n",
        "test_dir = os.path.join(dataset_dir, 'Test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Freeze the layers of VGG16 base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c-2w7irm8sSw"
      },
      "outputs": [],
      "source": [
        "# Add custom layers on top of the base model\n",
        "x = base_model.output\n",
        "# Adding custom convolutional layers\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "# Flatten and add dense layers\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(2, activation='softmax')(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting the image size and batch size\n",
        "img_size = (150, 150)\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the final model\n",
        "vgg_model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Vh-32ilo8sSw"
      },
      "outputs": [],
      "source": [
        "# Setting the path to our project folder\n",
        "project_folder = \"Dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6633 images belonging to 2 classes.\n",
            "Found 3765 images belonging to 2 classes.\n",
            "Found 3469 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_dataset = PyDataset(train_dir, batch_size, img_size, mode='train')\n",
        "validation_dataset = PyDataset(validation_dir, batch_size, img_size, mode='validation')\n",
        "test_dataset = PyDataset(test_dir, batch_size, img_size, mode='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhmSgQBG8sSw",
        "outputId": "83373525-621b-497b-f742-5a6a03408d38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6633 images belonging to 2 classes.\n",
            "Found 3765 images belonging to 2 classes.\n",
            "Found 3469 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        " # Oversampling 'bald'\n",
        "preprocessing_function=lambda x: x if np.random.choice([True, False], p=[0.5, 0.5]) else np.flipud(x)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    project_folder + '/Train',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    project_folder + '/Validation',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    project_folder + '/Test',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=64,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TAFaQ7Su8sSx"
      },
      "outputs": [],
      "source": [
        "# Get the number of samples in train and validation datasets\n",
        "num_train_samples = train_generator.samples\n",
        "num_val_samples = validation_generator.samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D0xIgWU68sSx"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "vgg_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DD84st1S8sSx"
      },
      "outputs": [],
      "source": [
        "# Hitung jumlah batch yang dihasilkan oleh generator untuk setiap epoch\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = validation_generator.samples // validation_generator.batch_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4He6H1O8sSx",
        "outputId": "772b65e8-e594-4382-b5cf-54a403751078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "  1/103 [..............................] - ETA: 13:09 - loss: 0.9674 - accuracy: 0.4062"
          ]
        }
      ],
      "source": [
        "#Training the model\n",
        "history = vgg_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK7Dlwbz8sSy",
        "outputId": "a187fa36-bf56-416d-979c-7991759fa2e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54/54 [==============================] - 222s 4s/step - loss: 0.2703 - accuracy: 0.9424\n",
            "Test Accuracy: 0.9424189925193787\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the model on the test set\n",
        "test_loss, test_acc = vgg_model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
        "print(\"Test Accuracy:\", test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49-VIbjr8sSy"
      },
      "outputs": [],
      "source": [
        "# Saving the model\n",
        "vgg_model.save(\"bald_detection_model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoGoo2jd4XaV"
      },
      "outputs": [],
      "source": [
        "# Simpan label\n",
        "class_names = ['Not Bald', 'Bald']\n",
        "with open('labels.txt', 'w') as f:\n",
        "    f.write('\\n'.join(class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfElx-EYFjR-"
      },
      "outputs": [],
      "source": [
        "# Function to detect baldness\n",
        "\n",
        "\n",
        "def detect_baldness(file_path):\n",
        "    global label1, label2  # Use global to access the label widgets\n",
        "\n",
        "    image = Image.open(file_path)\n",
        "    image = Image.resize((224, 224))\n",
        "    image_array = np.array(image) / 255.0\n",
        "    image_array = np.expand_dims(image_array, axis=0)\n",
        "\n",
        "    # Use the processed image data for prediction\n",
        "    predictions = vgg_model.predict(image_array)\n",
        "\n",
        "    # Access the prediction (assuming binary classification)\n",
        "    prediction = predictions[0][0]\n",
        "\n",
        "    # Update the labels with the prediction results\n",
        "    label1.configure(foreground=\"#011638\", text=\"Not Bald\" if prediction > 0.5 else \"Bald\")\n",
        "    label2.configure(foreground=\"#011638\", text=f\"Probability: {prediction:.4f}\")\n",
        "\n",
        "# Example usage:\n",
        "# detect_baldness('path_to_image.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK8qY1XMrAsc",
        "outputId": "ed074480-1153-40f1-f3e2-a747ea3053a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023468B035B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 373ms/step\n",
            "Bald\n"
          ]
        }
      ],
      "source": [
        "# Path to the image file\n",
        "image_path = 'Dataset/download.jpeg'\n",
        "\n",
        "# Load the image and resize it to the input shape of your model\n",
        "img = image.load_img(image_path, target_size=(224, 224))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# Preprocess the image (normalize pixel values)\n",
        "img_array = img_array / 255.0\n",
        "\n",
        "# Load the model\n",
        "model_path = 'bald_detection_model.keras'\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Make predictions\n",
        "predictions = vgg_model.predict(img_array)\n",
        "\n",
        "# Get the predicted class (0 for bald, 1 for not bald)\n",
        "predicted_class = np.argmax(predictions)\n",
        "\n",
        "# Print the predicted class\n",
        "if predicted_class == 0:\n",
        "    print(\"Bald\")\n",
        "else:\n",
        "    print(\"Not Bald\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 168ms/step\n",
            "Bald\n"
          ]
        }
      ],
      "source": [
        "# Path to the image file\n",
        "image_path = 'Dataset/Mendes.jpeg'\n",
        "\n",
        "# Load the image and resize it to the input shape of your model\n",
        "img = image.load_img(image_path, target_size=(224, 224))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# Preprocess the image (normalize pixel values)\n",
        "img_array = img_array / 255.0\n",
        "\n",
        "# Load the model\n",
        "model_path = 'bald_detection_model.keras'\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Make predictions\n",
        "predictions = vgg_model.predict(img_array)\n",
        "\n",
        "# Get the predicted class (0 for bald, 1 for not bald)\n",
        "predicted_class = np.argmax(predictions)\n",
        "\n",
        "# Print the predicted class\n",
        "if predicted_class == 0:\n",
        "    print(\"Bald\")\n",
        "else:\n",
        "    print(\"Not Bald\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmpt1rvg9o8\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmpt1rvg9o8\\assets\n"
          ]
        }
      ],
      "source": [
        "# Convert the model to TFLite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(vgg_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "tflite_path = \"assets/brain.tflite\"\n",
        "with open(tflite_path, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
